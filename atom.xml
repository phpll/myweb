<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://newdy.cf</id>
    <title>Gridea</title>
    <updated>2020-02-11T15:04:25.452Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://newdy.cf"/>
    <link rel="self" href="https://newdy.cf/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://newdy.cf/images/avatar.png</logo>
    <icon>https://newdy.cf/favicon.ico</icon>
    <rights>All rights reserved 2020, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[群晖存储空间损毁 Btrfs 数据恢复教程]]></title>
        <id>https://newdy.cf/post/qun-hui-cun-chu-kong-jian-sun-hui-btrfs-shu-ju-hui-fu-jiao-cheng</id>
        <link href="https://newdy.cf/post/qun-hui-cun-chu-kong-jian-sun-hui-btrfs-shu-ju-hui-fu-jiao-cheng">
        </link>
        <updated>2020-02-11T14:52:53.000Z</updated>
        <content type="html"><![CDATA[<p>由于囊中羞涩，reizhi 一直在使用黑群晖作为家庭存储方案。不知何故，几天前突然提示存储空间已损毁。这种情况下白群晖是可以直接联系技术支持的，无奈我只好自己想办法解决。而网络上搜索到的教程和案例都是使用 Ext4 作为文件系统，那么只需要用 UFS explorer 来修复就好了。偏偏我是用的是 Btrfs 文件系统，于是只好爬问研究。最终通过三天时间的反复尝试，成功将所有数据挽回，在此分享一下经历和经验供日后参考。</p>
<p>如果你也遇到了类似问题，完全不用急着慌张。虽然 Btrfs 相比于 Ext4 并没有任何稳定性上的优势，但经过多年的更新和改进文件系统已经比较完善，再加上 RAID 的数据保护，丢失文件的几率并不高。</p>
<p>如果你的群晖提示存储空间损毁，但 RAID 并没有异常，可以无需进行 RAID 清理。通过查看 S.M.A.R.T 状态，发现所有硬盘均处于健康状态，于是跳过这一步。接下来我们需要引导到 Ubuntu 系统并尝试挂载 RAID ，此时既可以使用原有机器，也可以将所有硬盘连接到其他机器中操作。在原机安装 Ubuntu 时请注意不要将系统安装至存有数据的硬盘。安装镜像以及教程可以直接在官网获取，这里便不再赘述了。另外由于恢复过程耗时较长，不建议使用 LiveCD 来操作。</p>
<p>安装完成后的第一件事是安装必要的工具包以及挂载 RAID，打开终端并以 root 身份（sudo -i）执行以下操作：</p>
<p>apt-get update<br>
apt-get install mdadm lvm2 btrfs-prog<br>
mdadm -Asf &amp;&amp; vgchange -ay<br>
正常完成后可以在磁盘管理中看到 RAID 阵列，但是由于文件系统损坏，此时是无法挂载的。这里会显示阵列的设备文件是 /dev/md/2 ，记住你的显示值，稍后会要用到。<br>
<img src="https://newdy.cf/post-images/1581432806246.png" alt="" loading="lazy"><br>
我们切换回终端，运行以下命令：</p>
<p>btrfs-find-root /dev/md/2 &amp;&gt; /tmp/root.txt<br>
运行过程可能需要10-30分钟，期间是没有任何回显的。等待运行完成后终端会返回命令提示符，这时我们打开 /tmp/root.txt 文件，可以看到如下内容：<br>
<img src="https://newdy.cf/post-images/1581432867060.png" alt="" loading="lazy"><br>
我们需要用到的数据是 Well block 后面的这一串数字，其后的 gen 数字越高，恢复的可能性越大。下一步使用找到的 tree root 来模拟修复，到目前为止的所有操作都不会对硬盘进行写入和修改，也不会破坏任何数据。</p>
<p>btrfs check --tree-root <block> --super <sup><br>
其中 <block> 为上一步中的数值，按 gen 数字从高到低依次尝试使用，<sup> 可以尝试0，1或2。如果 <block> 有效，运行结果末尾应当类似于以下图示：<br>
<img src="https://newdy.cf/post-images/1581432909952.png" alt="" loading="lazy"><br>
如果最后回显不是以上格式，表明这一条 <block> 无效，需要继续尝试下一条。在确认看到以上提示后，我们尝试将数据导出。</p>
<p>btrfs restore /dev/md/2 /tmp -D -v -F -i -t <sup><br>
此时仍然使用上一步中的 <block> 值，将 /tmp 改为导出目录，需要确保留有足够空间存储文件。如果文件名包含特殊符号可能导致导出中断，将目标分区格式化为 Ext3/4 即可。<br>
<img src="https://newdy.cf/post-images/1581432941261.png" alt="" loading="lazy"><br>
如果导出正常进行，会看到类似上图的提示，此处没有进度提示，可以自行前往导出目录查看。如果导出失败会给出其他提示，在确认导出分区是 Ext3/4 的情况下，则只能退回上一步尝试其他 <sup> 值。</p>
<p>到目前为止我们并没有对数据盘进行任何写入和修改操作，如果因为种种原因无法导出，或是导出过程异常中断，仍然可以通过修复原盘的方式来挽回数据。不过请注意，此步骤有可能会损坏数据，如果你不能接受任何风险，请停止执行并联系专业机构。</p>
<p>btrfs check --repair --tree-root <block> --super <sup><br>
使用之前步骤中正常回显的 <block> 及 <sup> 值进行正式修复，确认操作完成后执行：</p>
<p>btrfs rescue super-recover /dev/md/2<br>
<img src="https://newdy.cf/post-images/1581432991454.png" alt="" loading="lazy"></p>
<p>提示确认目标分区是 Btrfs 文件系统，否则会损坏数据，输入 y 确认操作。等待数秒后再次回到提示符，如果一切顺利，此时已经可以通过磁盘管理工具挂载 Btrfs 分区了。不过群晖很大几率不会识别修复后的文件系统，还是建议将数据导出后再将硬盘还原。😚</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[群晖开启 SMB3 多通道叠加网卡速度]]></title>
        <id>https://newdy.cf/post/qun-hui-kai-qi-smb3-duo-tong-dao-die-jia-wang-qia-su-du</id>
        <link href="https://newdy.cf/post/qun-hui-kai-qi-smb3-duo-tong-dao-die-jia-wang-qia-su-du">
        </link>
        <updated>2020-02-11T14:47:10.000Z</updated>
        <content type="html"><![CDATA[<p>不少的群晖机型都带有2个甚至更多的网卡，为了突破 1Gbps 的局域网连接速度，我们以往需要在交换机、群晖和 PC 端配置链路聚合(link aggregation)。不仅配置较为繁琐，而且额外增加了 PC 端和交换机的成本，实际普及率并不高。而自 DSM 6.1-15047 之后，群晖为我们带来了 SMB3 多通道支持，使得我们能够以及其低廉的成本享受多网卡叠加带来的速度提升。</p>
<p>要使用 SMB3.0 的多通道来叠加网卡速度，需要以下几个条件：</p>
<p>群晖带有2个或以上的相同线速的网卡，并安装 DSM 6.1-15047 及更高版本<br>
普通交换机<br>
PC 端带有2个或以上的相同线速的网卡，并安装 Windows 8/Server 2012及更高版本 OS<br>
由于群晖目前没有开放相关设置项，所以我们部分操作需要通过 SSH 来完成：</p>
<p>1.打开控制面板，依次进入：文件服务-高级设置，将最大协议改为 SMB3，点击应用<br>
<img src="https://newdy.cf/post-images/1581432465559.png" alt="" loading="lazy"><br>
2.进入：终端机和 SNMP，勾选启动 SSH 功能，点击应用<br>
<img src="https://newdy.cf/post-images/1581432503103.png" alt="" loading="lazy"><br>
3.使用 PUTTY 等软件登入群晖 SSH，输入 sudo -i 临时提权，并输入密码回车。待命令提示符由 $ 变为 # 后，执行：vi /etc/samba/smb.conf<br>
<img src="https://newdy.cf/post-images/1581432535629.png" alt="" loading="lazy"><br>
4.在文件末尾添加以下内容，完成后按 ESC，并输入 :wq 保存退出</p>
<p>server multi channel support = yes<br>
aio read size = 1<br>
aio write size = 1<br>
5.重启群晖，PC 和交换机，所有设置完成。</p>
<p>在重启完成后，PC 端使用主机名（如\homeshare）或 ip 进入一次共享，即可自动启用 SMB3 多通道了。另外也可以用管理员权限运行 PowerShell ，执行： Get-SmbMultichannelConnection 确认。<br>
<img src="https://newdy.cf/post-images/1581432576355.png" alt="" loading="lazy"><br>
如上所示，目前 PC 已连接到服务器 192.168.199.189，并且分别通过本地 ip:192.168.199.99 192.168.199.100 与远端 ip: 192.168.199.188 192.168.199.189 建立了 SMB3 多通道连接。<br>
<img src="https://newdy.cf/post-images/1581432604857.png" alt="" loading="lazy"><br>
通过复制文件也可以看出，两张网卡均有流量，并且总和超过了 1Gbps 。</p>
<p>基于 RTL8111 的 PCIE x1 网卡目前均价不过20-30块，并且对于交换机并无特殊限制，故整套解决方案成本是非常低的。除了PCIE 网卡之外，也可以使用 USB 网卡，但需要确保所有网卡线速一致，才能够启用 SMB3 多通道。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[为 ESXi 添加 RTL8168/8111 及 SATA 控制器驱动]]></title>
        <id>https://newdy.cf/post/wei-esxi-tian-jia-rtl81688111-ji-sata-kong-zhi-qi-qu-dong</id>
        <link href="https://newdy.cf/post/wei-esxi-tian-jia-rtl81688111-ji-sata-kong-zhi-qi-qu-dong">
        </link>
        <updated>2020-02-11T14:27:41.000Z</updated>
        <content type="html"><![CDATA[<p>ESXi 是 VMware 旗下的一款硬件虚拟化产品，用于快速搭建虚拟化平台。由于其免费易用的特点，ESXi 受到了个人玩家、开发者及小型公司的广泛好评。不过也许是考虑到产品线与 Workstation 和 Fusion 有或多或少的重合，自 ESXi 5.5 版本起 VMware 移除了大量家用 PC 机的驱动，其中就包含 RTL8168/8111 网卡以及 Intel SATA 控制器。</p>
<p>不过天无绝人之路，我们仍然可以使用第三方工具 ESXi-Customizer-PS 对安装镜像进行修改，加载我们想要的驱动，从而继续正常使用。</p>
<p>在使用 ESXi-Customizer-PS 之前，我们需要先安装 VMware PowerCLI，可以在网络搜索下载。安装完成后，在这个网址下载 ESXi-Customizer-PS。</p>
<p>随后我们打开 VMware PowerCLI ，并且定位到 ESXi-Customizer-PS 所在的目录，执行：</p>
<p>##.\ESXi-Customizer-PS-v2.5.ps1 -v65 -vft -load net55-r8168,net51-r8169,sata-xahci<br>
<img src="https://newdy.cf/post-images/1581431304708.png" alt="" loading="lazy"></p>
<pre><code>随后脚本会自动连接到 VMware 下载安装镜像并加载对应的驱动。其中 net55-r8168 对应 RTL8168/8111 ，net51-r8169 对应 RTL8169 ，sata-xahci 对应常见 SATA 控制器。如果你还有特殊的硬件驱动需要加载的话，可以到 V-Front 的官方源按照硬件 id 来查找，只需在 load 后继续添加并用逗号分隔即可。
</code></pre>
<p>经过漫长的等待后，对应版本的 ESXi 安装镜像便会保存在 ESXi-Customizer-PS 同目录中。当然你也可以使用 -v60 -v55 来生成6.0或是5.5版本的安装镜像。<br>
<img src="https://newdy.cf/post-images/1581431377324.png" alt="" loading="lazy"><br>
如不清楚硬件 id ，可先引导至 Windows 并通过设备管理器查看。如图所示，这个 Intel USB 3.0 控制器的硬件 id 为：8086:A12F。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[搭建 DNS 服务解锁 Netflix]]></title>
        <id>https://newdy.cf/post/da-jian-dns-fu-wu-jie-suo-netflix</id>
        <link href="https://newdy.cf/post/da-jian-dns-fu-wu-jie-suo-netflix">
        </link>
        <updated>2020-02-11T13:14:06.000Z</updated>
        <content type="html"><![CDATA[<p>有观看 Netflix 习惯的朋友应该都知道，除了需要科学上网之外，IP 能够解锁 Netflix 也是正常播放的必要条件之一。不过有时因为流量原因，我们不直接使用能够解锁 NF 的服务器来播放，而是架设 DNS 服务作为其它机器的中转。这样，原本不能解锁 NF 的服务器便也可以正常播放了。不过我们今天并不细究原理，只是分享方法。</p>
<p>如果需要解锁 Netflix ，前提条件是你拥有一台能够正常观看 NF 的服务器A，以及另一台无法观看 NF 的服务器B。</p>
<p>首先我们在服务器A上运行以下代码安装 DNS 服务：</p>
<p>wget --no-check-certificate -O dnsmasq_sniproxy.sh https://raw.githubusercontent.com/myxuchangbin/dnsmasq_sniproxy_install/master/dnsmasq_sniproxy.sh &amp;&amp; bash dnsmasq_sniproxy.sh -i</p>
<p>如果开启了防火墙，记得在服务器A上放行53端口的入网连接。随后在服务器B上设置 DNS 为服务器A的 ip ，以 Debian 为例：</p>
<p>如果你的服务器B是静态配置 IP，只需要修改<br>
/etc/resolv.conf 中的 nameserver 即可。如：nameserver 1.0.0.1</p>
<p>如果你的服务器B是自动获取IP，切勿修改 /etc/resolv.conf 。<br>
因为此时这个文件受到 DHCP 的影响，在每次重启后都会还原为默认值。此时我们需要编辑 /etc/dhcp/dhclient.conf ，添加：supersede domain-name-servers 1.0.0.1; 来手动指派 DNS 地址。</p>
<p>最后重启服务器B的网络即可：/etc/init.d/networking restart</p>
<p>上述例子中的 1.0.0.1 仅用于示范，实际使用时需要填写服务器A的公网 ip 地址。</p>
<p>此时再使用服务器B，Netflix 已经可以正常播放了。并且 DNS 解锁只需要消耗服务器A非常少的流量，适用于流量较少无法直接使用服务器A来观看 NF 的情况。为了提高系统安全性，还可以自行配置 iptables 来限制53端口的入网 ip 地址，在此便不细谈了。</p>
]]></content>
    </entry>
</feed>